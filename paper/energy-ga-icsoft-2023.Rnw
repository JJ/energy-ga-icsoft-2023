\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage[bottom]{footmisc}
\usepackage{hyperref}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.

\newcommand{\pinp}{\texttt{pinpoint}}
\newcommand{\lik}{\texttt{likwid}}


\begin{document}

\title{An analysis of energy consumption of JavaScript interpreters with evolutionary algorithm workloads}

\author{\authorname{Juan J. Merelo-Guervós\sup{1}\orcidAuthor{0000-0002-1385-9741} and Mario García-Valdez\sup{2}\orcidAuthor{0000-0002-2593-1114}}
\affiliation{\sup{1}Department of Computer Engineering, Automatics and Robotics, University of Granada, Granada, Spain}
\affiliation{\sup{2}Department of Graduate Studies, National Technological Institute of Mexico, Tijuana, Mexico}
\email{jmerelo@ugr.es, mario@tectijuana.edu.mx}
}

\keywords{Green computing, metaheuristics, JavaScript}

\abstract{
Energy-aware computing includes many different variables and parameters, which makes it necessary to focus on a single one to obtain meaningful results. In this paper, we will look at the energy consumption of three different JavaScript interpreters: {\sf bun}, {\sf node} and {\sf deno}; given their different conceptual designs, we should expect different energy budgets for running the (roughly) same workload, operations related to evolutionary algorithms, a population-based stochastic optimization algorithm. In this paper we will first test different tools to measure per-process energy consumption in a precise way, trying to find the one that gives the most accurate estimation; after choosing the tool by performing different experiments on a workload similar to the one carried out by evolutionary algorithms, we will focus on EA-specific functions and operators and measure how much energy they consume for different problem sizes. From this, we will try to draw a conclusion on which JavaScript interpreter should be used in this kind of workloads if energy (or related expenses) has a limited budget.
}

\onecolumn \maketitle \normalsize \setcounter{footnote}{0} \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

From a language designed in the nineties for simple browser widgets and
client-side validations \cite{goodman2007javascript,flanagan1998javascript},
JavaScript is nowadays the language most widely used by developers in their
GitHub repositories \cite{ogrady22:ranking}, occupying this position since 2014
\cite{ogrady14:ranking}, mainly because it is almost exclusively the language
needed for front-end programming (competing only with app development languages,
such as Swift or Kotlin, or languages transpiled to JavaScript, such as Dart),
while at the same time being strong for full-stack development, with solid
support for the back end, including application servers, middleware, and
database programming. Other popularity indices, such as
TIOBE\footnote{\url{https://www.tiobe.com/tiobe-index/}}, that take into account other
factors besides lines of code production, currently (2023) rank it as the seventh,
although it was also the most popular language in 2014. It can
be claimed, then, that it is among the most popular, if not the most popular
language in current development.

Due to its popularity and the fact that it has a
continuously evolving standard \cite{ecma1999262}, traditionally, there have
been different virtual machines (or interpreters) to run its programs. During
the first years, browsers were the only running platform available; however, the
introduction of Node.js running on the V8 JavaScript Engine \cite{5617064} gave
it the popularity it has today; this popularity, in turn, provoked new
interpreters to spawn like {\sf deno} \cite{runtimeintroducing} (written in
Rust) and {\sf bun} \cite{bun} programmed in the relatively unknown language
Zig.

No wonder, then, that JavaScript is also a popular language for implementing
metaheuristics, especially evolutionary algorithms. Evolutionary algorithms \cite{eiben2015evolutionary} are population-based stochastic optimization algorithms based on the representation of a problem as a (often binary) "chromosome", and {\em evolution} of population of these "chromosomes" by random change (via "genetic" operators, mutation and crossover) and survival of the fittest (evaluation of those chromosomes via a so-called "fitness" function, and selection and reproduction of those that achieve the best values).
From the early
implementations in the browser
\cite{smith1996ga,jesusIWANN99,langdon2004global}, whole libraries
\cite{EvoStar2014:jsEO}, through complete implementations geared towards
volunteer computing \cite{2016arXiv160101607M}. However, one of the criticisms
leveraged towards these implementations is the (possible) lack of speed when
compared to other compiled languages (mainly Java, very popular in
metaheuristics implementations, or C++).

This is why, since implementation matters \cite{DBLP:conf/iwann/MereloRACML11},
choosing the right interpreter is going to have a significant impact on the performance
of any workload; if you decide to choose JavaScript for any reason (such as
seamless client/server integration, or be able to run your algorithm either on
the browser or from the command line if desired) knowing which VM delivers the
best performance is essential, either from the scientific, or software engineering
points of view.

At the same time, with the advent of the concept of green computing
\cite{kurp2008green}, it becomes increasingly important to measure not only the
raw wall clock performance (which was the focus of papers such as
\cite{DBLP:conf/evoW/GuervosBCRGRVHR17}), but also to achieve a certain level
of performance with a certain amount of energy consumption, or else to minimize
the consumption needed to run a certain workload. This will be the main focus
of this paper; since the core of the different JS virtual machines is
different, and are created with languages with different focus (Rust is focused
on memory safety \cite{noseda2022rust}, Zig based on simplicity and performance
\cite{zig}), different energy consumption should be expected. Since all three
languages can (roughly) run the same, unmodified source code, what we intend
with this paper is to advise on which JS interpreter might give the lowest
power consumption, the maximum performance, or both, so that evolutionary
algorithm practitioners can target it for their development.

The rest of the paper follows this plan: next we will present the state of the
art; then we will describe the experimental setup in Section \ref{sec:setup};
results will be presented next in Section \ref{sec:res}, and we will end with a
discussion of results, conclusions and future lines of work.

\section{State of the art}

% First, CPU and general algorithmic impact on power consumption

The power efficiency of CPUs (computations per kilowatt-hour) has
doubled roughly every year and a half from 1946 to 2009
\cite{koomey2011web}, this improvement has been mainly a by-product
of Moore's law, the trend of chip manufacturers to decrease in half
the size and distance between transistors every two
years. Unfortunately, it is expected that physical limits of
electronics will slow down this miniaturization in the near future.
Nonetheless, energy efficiency is becoming the most important metric
of performance and selling point in hardware development, and it is an
important driver for current innovation. The challenge of building
more power-efficient systems, can be addressed at the hardware and
software levels. In the software level, developers focus their
attention on the energy consumption of software, proposing
optimizations for more energy-efficient algorithm implementations.
Algorithm comparatives nowadays include power efficiency as a
performance metric, these include encryption algorithms
\cite{mota2017comparative,thakor2021lightweight}, estimation models
for machine learning applications \cite{garcia2019estimation} and
genetic programming (GP) \cite{diaz2018fuzzy}, and code refactoring
\cite{ournani2021tales}. Since metaheuristics are so extensively used
in machine learning applications, its interest in research has grown
in parallel to its number of applications. Many papers focus on
analyzing how certain metaheuristics parameters have an impact on
energy consumption.  Díaz-Álvarez et al. \cite{diaz2022population}
studies how the populations size of evolutionary algorithms (EAs)
influences power consumption. In an earlier work centered on genetic
algorithms (GAs) \cite{10.1007/978-3-030-45715-0_8} power-consumption
of battery-powered devices was measured for various parameter
configurations including chromosome and population sizes. The
experiments used the one max and trap function benchmark problems,
they concluded that execution time and energy consumption do not
linearly correlate and there is a connection between the GA parameters
and power consumption.  In GAs, the mutation operator appears to be a
power-hungry component according to Abdelhafez et
al. \cite{abdelhafez2019component}, in their paper they also report
that in a distributed evaluation setting, the communication scheme has
a grater impact. Fernández de Vega et
al. \cite{10.1007/978-3-319-45823-6_51} experimented with different
parameters for a GP algorithm and concluded that hand-held devices and
single-board computers (SBCs) required an order of magnitude less
energy to run the same algorithm.


\section{Methodology and experimental setup}
\label{sec:setup}

There are many ways to measure the consumption of applications running in a
computer; besides measuring directly from the power intake, those running as
applications and tapping the computer sensors fall roughly into two fields:
power monitors and energy profilers \cite{cruz21}. Power monitors need
additional hardware to measure the power drawn by the whole machine;
besides being expensive, their setup is difficult, and it is complicated 
to measure precisely how much a specific process consumes.

On the other hand, energy profilers are programs that draw information from
hardware sensors \cite{sinha2001jouletrack}, generally exposed through kernel
calls or higher-level wrapper libraries, to pinpoint consumption by specific processes
during in a time range. Tools that give these measures, either with a
graphic or a command line interface, have been available for some time already,
and have become more popular lately. One of the mainstream processor architectures,
Intel, includes an interface called RAPL, or Running Average Power Limit
\cite{rapl}. Essentially, it consists of a series of machine-specific registers
(MSRs) that contain information on the wattage drawn by different parts of the
architecture; the content of these registers will be processed (through the
corresponding library) and consumed by different command line utilities. We will
use these command line utilities since they produce an output that can be
automatically processed and evaluated, which is what we are looking for in this
paper\footnote{Systems based on the AMD architecture have a similar power
  profiling system called AMP with its corresponding command line
  tool. However, we found that it was not well documented, and excessively
  complicated for the purposes of this article. Although not as complete, AMD
  processors also include the aforementioned MSRs so RAPL-based utilities can
  run on them}.

Energy profiling, as measured by RAPL or other APIs, includes different
\emph{domains} \cite{khan2015energy}, essentially the computing devices or
peripherals requiring the reported amount of energy.
DRAM or dynamic RAM, CORE, or GPU will report what happens on those specific devices,
with a core being every one of the computing units within the central processing
unit; other domains, like PKG or package, will report what happens in the
``package'', or CPU together with other devices in the chipset.

Considering that the available system has an AMD architecture, which is
roughly compatible with the RAPL architecture, we will use two command line
utilities, as they are the only ones available that can wrap the execution of a command
and report on consumption for that specific command. These are open source tools
that can be obtained directly from the corresponding repositories or by downloading and
compiling their source code. \begin{itemize}
\item {\sf pinpoint}~\cite{9307947} (available from
  \url{https://github.com/osmhpi/pinpoint}) is a tool that uses the RAPL
  interface, as well as the NVIDIA registers, to report the power consumed by
  these devices. In this paper, we will use it since it is the only one out of
  the three tools that can show the GPU consumption.
\item {\sf
    perf}\footnote{\url{https://perf.wiki.kernel.org/index.php/Main_Page}}~\cite{treibig2010likwid} is a
  system tool that measures all kinds of performance events, including power
  consumption. It will be used mainly for the  \texttt{pkg} domain; this is a
  domain that it is measured by all tools, but all of them would process device
    readings differently, so it will provide us with alternative estimations of
    the consumption. 
  \item {\sf likwid-powermeter}\footnote{\url{https://github.com/RRZE-HPC/likwid}}
  measures the \texttt{CORE} as well as the \texttt{PKG} domain, which includes
  the former together with the so-called ``uncore'' components.
\end{itemize}

On the JavaScript side, we used three different interpreters:\begin{itemize}
\item \texttt{bun} version 0.5.8
\item \texttt{deno} version 1.32.1, which includes the v8 library version
  11.2.214.9 and typescript 5.0.2
\item  \texttt{node.js} version 18.5.0
\end{itemize}

\texttt{bun} and \texttt{nodejs} are fully compatible, so they run exactly the
same code. The code for \texttt{deno} needed a small modification: the path to
the library had to be changed (since it does not use the \texttt{node\_modules}
to host installed modules), and it uses a different library for processing 
the command line arguments. Other than that, the business logic was exactly the same.

These were running in an Ubuntu version 20.04.1 with kernel version
5.15.0-69. The processor is an AMD Ryzen 9 3950X 16-Core. Since we will
not be testing in a pure Intel architecture, the complete RAPL API is
not going to be available; that is also why we will be experimenting
with different tools so that we can have an adequate coverage
of energy consumption for the commands we will be measuring.

A Perl script was created to perform the experiments; it launched the scripts
and collected results by analyzing the standard output and putting it in a CSV
format that would allow examination of the experiments.

The initial experiment consisted in a script that used the \texttt{saco-js}
library to perform the union of ``bags'', sets that can hold several copies of
the same item. 1024 sets were generated; these sets had 1024, 2048, 4096
elements. Then, a union of bags was performed on pairs of sets until
there was only one left. This is similar to some operations performed by
evolutionary algorithms, mainly related to merging populations. They do not
involve floating point operations in any way.

The scripts to launch every kind of tool were slightly different, mainly because
the output needed to be processed in different ways (and different kinds of
information extracted). Additionally, \texttt{pinpoint}, which is the only tool
that does not need superuser privileges, sometimes returned 0 in energy
measures. This was an error, and those runs were discarded.

Finally, the scripts performed an additional task: since it is not possible to
disaggregate the readings for our program from the energy consumed by other
processes running at the same time, what we did was to run every program 15
times, compute the average time, and then use the same tool to measure the
energy consumption for the \texttt{sleep} program during the average amount of
time. The energy readings shown are the result of subtracting this measurement
from every one of the 15 other measurements taken, so that we can analyze
the differential of energy that has been consumed by our programs; the result is clipped at 0, since negative energy differentials would make no sense.

<<r pinpoint, echo=F>>=
library(ggplot2)
test.pinpoint <- read.csv("../code/data/pinpoint-sets-29-Mar-21-06-12.csv")
@

Experimenting with this simple program will mainly allow us to calibrate the
different tools in order to pick only one, if possible, as well as validate the
program and iron out all possible errors in the program itself or the
processing scripts.

<<r pinpoint.gpu, echo=F, fig.cap="Boxplot of measurements of energy expenditures by the GPU in the sets problem. GPU energy consumption is measured in Joules.\\protect\\label{fig:gpu}">>=
test.pinpoint$size <- as.factor(test.pinpoint$size)
ggplot(test.pinpoint, aes(x=size,y=GPU,group=size))+geom_boxplot()
@

One of the first observations we can draw from these initial experiments is
whether measure how much energy the GPU spends it is interesting. Since
\texttt{pinpoint} is the only tool able to measure this, we will use it. We
show the results in Figure ref{fig:gpu}. We see here that it is mostly
independent of the set size, but most importantly, it is 0 in most cases,
more than 50\%. The rest must be essentially noise, amounting to a few
Joules anyway. In order to decide if we can use this tool alone or
complement it with other measures, we need to validate or enhance its
measurements with others; that is why next, we will make some test measurements
with the next tool, \lik{}.

<<r likwid, echo=F, fig.cap="Boxplot of (differential) measurements of energy consumption as measured by the CORE and PKG (package) registers, both measured in Joules.\\protect\\label{fig:likwid}">>=
test.likwid <- read.csv("../code/data/likwid-sets-10-Apr-12-54-28.csv")
test.likwid$size <- as.factor(test.likwid$size)
ggplot(test.likwid, aes(x=size,y=CORE,group=size))+geom_boxplot()
ggplot(test.likwid, aes(x=size,y=PKG,group=size))+geom_boxplot()
@

Another tool that we have tested, \texttt{likwid}, can also measure the
\emph{package} energy consumption. In this initial exploration, we will 
see whether this measurement is significant and to what extent it is related
to the core measurements. Boxplots of energy consumption for different set size
is shown in Figure \ref{fig:likwid} for the CORE (top) and PKG RAPL registers.
It seems to show that differential energy consumption (once the baseline has been
subtracted) is almost negligible in the CORE register unless the size is big
enough (4096) in both cases, although the CORE register shows a certain amount
of consumption for size = 2048. It is entirely unlikely that these kinds of
measurements have a certain degree of uncertainty; however, apparently, \pinp
will directly estimate these runs as 0, and thus are skipped; however, this
tool does measure a certain amount of consumption that cannot be so easily
filtered.
%
<<r pkg.core, echo=F, fig.cap="Relationship between CORE and PACKAGE consumption (x and y axis, respectively.\\protect\\label{fig:corepkg}">>=
ggplot(test.likwid, aes(x=PKG,y=CORE,color=size))+geom_point()
@
%
Plotting the relationship between these two registers (see Figure \ref{fig:corepkg}), however, shows a linear relationship, so we do not really need to plot both. One of them will be enough, and since PKG seems to have the greatest variation, we will stick to that one.

<<r pinpoint.likwid, echo=F, fig.cap="Relationship between PKG measurements taken by pinpoint and likwid.\\protect\\label{fig:pinlik}">>=
pinlik <- data.frame(tool=c(rep("pinpoint",45),rep("likwid",48)),
                     size=c(c(rep("1024",15),rep("2048",15),rep("4096",15)),c(rep("1024",16),rep("2048",16),rep("4096",16))),
                     PKG=c(test.pinpoint$PKG,test.likwid$PKG))
ggplot(pinlik, aes(x=tool,y=PKG))+geom_boxplot(aes(fill=size))
@

Since several tools are available to measure PKG, we need to find out
what kind of measurements they have, and if there is a correlation or even
equality between them. We show how PKG energy consumption is registered by
\pinp{} and \lik{} in Figure \ref{fig:pinlik}. We have already seen in Figure
\ref{fig:likwid} how the measurements registered by \lik{} have some strange
behavior; this chart shows that while \pinp{} shows a reasonable amount of
consumption for all three sizes, \lik{} just registers zero, which is not
reasonable in this case. This will lead us to discard the use of this tool in
the upcoming experiments.

<<r perf, echo=F, fig.cap="Relationship between PKG measurements taken by pinpoint and perf, with boxplots with values for the three set sizes.\\protect\\label{fig:pinperf}">>=
test.perf <- read.csv("../code/data/perfstat-deno-sets-30-Mar-14-51-25.csv")
pinperf <- data.frame(tool=c(rep("pinpoint",45),rep("perf",47)),
                     size=c(c(rep("1024",15),rep("2048",15),rep("4096",15)),c(rep("1024",16),rep("2048",16),rep("4096",15))),
                     PKG=c(test.pinpoint$PKG,test.perf$PKG))
ggplot(pinperf, aes(x=tool,y=PKG))+geom_boxplot(aes(fill=size))
@

Unlike what happened with the previous tool, Figure \ref{fig:pinperf} shows
that there is a certain agreement between these two, except for the fact that
\texttt{perf} seems to measure exceptionally low values in the lowest sizes. A
Wilcox test shows significant differences for all three sizes; however, this
might be due to a different amount of overhead or other unknown
environmental factors.

<<r sets.vms, echo=F, fig.cap="PKG measurements for the set problem and the three different virtual machines.\\protect\\label{fig:testvms}">>=
sets.vms <- read.csv("../code/data/pinpoint-vms-sets-10-Apr-17-59-56.csv")
sets.vms$size <- as.factor(sets.vms$size)
ggplot(sets.vms, aes(x=size,y=PKG))+geom_boxplot(aes(fill=VM))
sets.vms$Joules.Second <- sets.vms$PKG / sets.vms$seconds
@

Eventually, since \pinp{} is able to measure all size ranges accurately, we will use it exclusively, as well as the quantity it measures, for comparison of different virtual machines. The initial results for this test drive are shown in Figure \ref{fig:testvms}. This is due, in part, to the fact that they take a different amount of time, so it would be interesting to find out whether the energy density, or the consumption of energy per second, is similar. This is shown in Figure \ref{fig:jouls}.

<<r sets.vms.jouls, echo=F, fig.cap="Boxplot of energy consumption per second, in Joules/s.\\protect\\label{fig:jouls}">>=
ggplot(sets.vms, aes(x=size,y=Joules.Second))+geom_boxplot(aes(fill=VM))
@

The interesting thing about this Figure is that the consumption per second
varies with the VM used, as well as the size. Remarkably, {\sf deno} keeps
it approximately constant, while {\sf bun} and {\sf node} exhibit a variation
with the size, and not in a systematic way. It is interesting also to note that
while {\sf bun}, in general, will spend less energy for each unit of work done than
{\sf node}, it will do so at the expense of exercising the CPU package more
strenuously, spending much more energy per second than the other two VMs.

These, however, are preliminary findings with a test problem; we will have to design experiments for actual operations used in evolutionary algorithms, which we will do next.

\section{Experimental results}
\label{sec:res}

<<r onemax, echo=F, fig.cap="Boxplot of energy consumption vs. time taken for all three sizes and VMs.\\protect\\label{fig:onemax}">>=
library(ggthemes)
onemax <- read.csv("../code/data/pinpoint-vms-onemax-11-Apr-10-19-29.csv")
onemax$size <- as.factor(onemax$size)
ggplot(onemax, aes(x=seconds,y=PKG))+geom_point(size=3,aes(color=size,fill=size,shape=VM))+theme_tufte()
@

As was done in \cite{DBLP:conf/evoW/MereloCBRGFRV16}, which was focused on
wallclock performance, the experiments will be focused on the key operations
performed by an evolutionary algorithm: evaluation of fitness and "genetic"
operators like mutation and crossover. What we will do is, repeat the setup
in the initial exploration, check the energy consumption for the processing of
40000 chromosomes, a number chosen to take a sizable amount of memory, but also
on the ballpark of the usual number of operations in an evolutionary algorithm
benchmark, it is also small enough to not create garbage collection problems
with the memory, something that was detected after the initial exploration.
Experiments were repeated for the same chromosome size as before, 1024, 2048,
and 4096, and for the three JS virtual machines used. Although the business
logic is exactly the same for the experiments, the script run comes in two
versions, one for {\sf deno} and the other for {\sf bun/node}, due to the different
way they have of reading command-line arguments. This does not affect the
overhead in any way. Code, as well as the data resulted from the experiments
and analyzed in this paper, are released with a free license (along with this
paper) from the repository \url{https://github.com/JJ/energy-ga-icsoft-2023}.

First, we will evaluate a typical fitness function, OneMax, which counts the
number of ones in a binary (1s and 0s) string. This type of functions, which
check the values of bits in a string and assign an integer value to it are
typical of many papers focused on evaluating evolutionary algorithms,
including parallel versions \cite{DBLP:conf/gecco/GuervosV18}.

The results are shown in Figure \ref{fig:onemax}. This already shows that time,
as well as energy consumption, for {\sf node} is higher; just check the
separation of the squares representing individual experiments in that
interpreter to the rest of the values for the same color; this separation
increases with chromosome size. But this paper focuses on energy consumption,
which we summarize next in Figure \ref{fig:onemax:energy}.

%
<<r onemax.vms, echo=F, fig.cap="Boxplot of PKG measurements OneMax problem and the three different virtual machines.\\protect\\label{fig:onemax:energy}">>=
ggplot(onemax, aes(x=size,y=PKG))+geom_boxplot(aes(fill=VM))+ylim(0, NA)+theme_tufte()
onemax$kwh <- onemax$PKG * 2.77778e-7
onemax$cost.Spain <- onemax$kwh * 20
@
%

The figure shows the almost-flat growth of energy consumption for {\sf bun}.
How consumption grows for {\sf deno} is weird, since it takes less energy when
the chromosome is bigger (4096). Once again, {\sf node} is the bigger energy
guzzler, consuming up to 3 times more than {\sf deno} on average, and more than
6 times as much as {\sf bun}. We will see how this is reflected in monetary
terms, taking into account that the cost in Spain today is around 0.2€/kWh.
This cost, shown in table \ref{tab:onemax:cost}, reaches almost one-hundredth
of a euro for the most "expensive" VM, {\sf node}; that gives you an idea of
the kind of cost the algorithms have, and also how this cost decreases almost
an order of magnitude if {\sf bun} is used.

<<r onemax.cost, echo=F, message=F>>=
library(dplyr)
onemax.cost <- onemax %>% group_by( size, VM) %>% summarise( average = mean( cost.Spain ), sd = sd( cost.Spain))
library(kableExtra)
kable(onemax.cost, caption="Estimated cost of the OneMax runs for every VM and size, in €-cents. \\protect\\label{tab:onemax:cost}")
@

The crossover operation involves copy operations between strings, as well as
creation of new strings. We will again generate 40K chromosomes and group them
in pairs; these strings will be crossed by interchanging a random
fragment from one to the other and back. The resulting pairs will be stored in
an array, which is eventually printed. The result of every experiment
is shown in an energy vs. wallclock time chart in Figure \ref{fig:xover}.

<<r crossover, echo=F, fig.cap="PKG consumption, in Joules, vs. time in seconds, for the crossover and the three different virtual machines.\\protect\\label{fig:xover}">>=
crossover <- read.csv("../code/data/pinpoint-vms-crossover-11-Apr-17-06-52.csv")
crossover$size <- as.factor(crossover$size)
ggplot(crossover, aes(x=seconds,y=PKG))+geom_point(size=3,aes(color=size,fill=size,shape=VM))+theme_tufte()
@

The scenario is remarkably similar to the one shown in Figure \ref{fig:onemax}.
In the two cases, {\sf bun} achieves the top performance and lowest energy
consumption, and {\sf node} is the worst. Average energy consumption is shown as a
boxplot in Figure \ref{fig:crossover:energy}.

%
<<r crossover.energy, echo=F, fig.cap="Boxplot of PKG measurements for the crossover operator and the three different virtual machines.\\protect\\label{fig:crossover:energy}">>=
ggplot(crossover, aes(x=size,y=PKG))+geom_boxplot(aes(fill=VM))+ylim(0, NA)+theme_tufte()
@
%

Here we can see again the surprising fact that {\sf deno} takes the same amount
of energy, on average, as {\sf node} for size 2048, in a similar case to what
happened for OneMax (shown in Figure \ref{fig:onemax:energy}). The difference
between the thriftiest, {\sf bun}, and the heaviest consumer, {\sf node}, is
approximately three times, in this case, less than in the case of the OneMax fitness
function.

Given that the results for the two evolutionary-algorithm-specific functions,
as well as the test function, are quite conclusive, we should not expect
anything different from the mutation function, so we will leave the experiments
at these two, and proceed to the conclusions.

\section{Conclusions}
\label{sec:conc}

In this paper, we set out to study the energy efficiency of different
JavaScript interpreters in evolutionary algorithm workloads, by studying how
short scripts that are extensively used in implementations of these algorithms
work and how these energy expenses scale with the chromosome size. We have
developed a methodology to accurately measure per-process consumption; we have
also adopted a tool, \pinp, that is able to give good estimations of sensor
readings, discarding experiments where that estimation was not adequate; this
tool has been calibrated by comparing its readings with other tools, which were
also evaluated for the same purpose and eventually discarded. We have also adopted a
benchmark-based approach, similar to the one used to measure performance, so
that consumption for specific operations can be pinpointed, discarding noise
produced by an implementation of a complete algorithm; that is, a whole program includes different operations, applied in different proportion, that might cancel each other. Testing short code paths, as proposed by \cite{10.1145/2425248.2425252} makes easier to understand their individual contribution to the overall consumption of the algorithm, and eventually optimize their specific code, or the number of times they are applied in the algorithm.
% I did not understand the last sentence. - M
% Clarified and added citation - JJ

During the exploratory data analysis, we have established that, in Linux
machines, \pinp{} can be profitably used to measure per-process energy
consumption, as long as these measurements are repeated and the process
themselves include short snippets of business logic; this tool should be
preferred over others that are either less accurate or simply take into account
different aspects of energy consumption.

The main point of this paper, however, was to check which JavaScript
interpreter should be used if our objective is to consume the minimum amount of
energy; the experiments have reliably confirmed {\sf bun} to be that tool. Not
only it consumes less for all the range of chromosome sizes used; it also takes
less time and can run applications written for Node (mostly) unmodified; its
consumption also scales better with problem size. This might be due to design
considerations, but also to the fact that the language used to write it, Zig,
emphasizes compile-time safety and manual memory allocation by default, and avoids hidden control flow. % JJ please check if the last sentence is correct. - M
% Changed to claims from the ziglang.org website - JJ
The only inconvenience of this interpreter is that it has not
reached version 1.0 yet, being currently in version 0.5.9; this might prevent
any company or organization from using it in production environments.

If that is an issue, {\sf deno} might be a good alternative. Except in a
specific case, it is going to be faster and consume less energy than {\sf
node}, even more so when memory requirements are high. According to our initial
exploration, it will also consume less energy {\em per second}, thus for
workloads that take roughly the same time, it will be a better candidate than
{\sf node}. As an inconvenience, it needs minor modifications to run, at least
if you need core or other kind of external libraries; its core library modules
are different to those used in {\sf node/bun}, although that need not be a
disadvantage per se.

The previous two points imply that, energy-wise, there are no good reasons to
use {\sf node.js} for running evolutionary algorithms. Except if the business logic
uses specific, early-adoption, or some features that, for some reason, does not
work with {\sf bun} yet, we would advise anyone to keep using {\sf bun} for this
kind of workloads.

As we have indicated in the experimental session, the wide advantage that {\sf
bun} has over the other interpreters does not leave much room for adopting
different benchmarks that could make that ranking vary; at any rate, these
experiments have shown how much faster and energy-saving {\sf bun} is (from 1/3
to 1/6 the energy consumed by {\sf node}), but it would be interesting to know
what happens to this gap under different operations like selection or different
kind of mutation. At the same time, even if evolutionary algorithms are mostly
GPU-free, there are some fitness functions that operate on floating point
numbers and thus would need to use the GPU; how interpreters work in this area
could be an interesting future line of work. This, along with testing different
versions of the interpreters as they are published, will be the subject of
future papers.



\section*{Acknowledgements}

This work is supported by the Ministerio espa\~{n}ol de Econom\'{\i}a y
Competitividad (Spanish Ministry of Competitivity and Economy) under project
PID2020-115570GB-C22 (DemocratAI::UGR).

\bibliographystyle{apalike}
{\small
\bibliography{energy,javascript,geneura}}


\end{document}

