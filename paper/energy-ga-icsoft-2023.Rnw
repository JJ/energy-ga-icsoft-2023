\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
\usepackage{subcaption}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multicol}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage[bottom]{footmisc}
\usepackage{hyperref}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.


\begin{document}

\title{An analysis of energy consumption of JavaScript interpreters/compilers in evolutionary algorithm workloads}

\author{\authorname{Juan J. Merelo-Guervós\sup{1}\orcidAuthor{0000-0002-1385-9741} and Mario García-Valdez\sup{2}\orcidAuthor{0000-0002-2593-1114}}
\affiliation{\sup{1}Department of Computer Engineering, Automatics and Robotics, University of Granada, Granada, Spain}
\affiliation{\sup{2}Department of Graduate Studies, National Technological Institute of Mexico, Tijuana, Mexico}
\email{jmerelo@ugr.es, mario@tectijuana.edu.mx}
}

\keywords{Green computing, metaheuristics, JavaScript}

\abstract{
% The abstract should summarize the contents of the paper and should contain at least 70 and at most 200 words. The text must be set to 9-point font size.
}

\onecolumn \maketitle \normalsize \setcounter{footnote}{0} \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}

From a language designed in the nineties for simple browser widgets and
client-side validations \cite{goodman2007javascript,flanagan1998javascript},
JavaScript is nowadays the language most widely used by developers in their
GitHub repositories \cite{ogrady22:ranking}, occupying this position since 2014
\cite{ogrady14:ranking}, mainly because it is almost exclusively the language
needed for front-end programming (competing only with app development languages,
such as Swift or Kotlin, or languages transpiled to JavaScript, such as Dart),
while at the same time being strong for full-stack development, with solid
support for the back end, including application servers, middleware, and
database programming. Other popularity indices, such as
TIOBE\footnote{\url{https://www.tiobe.com/tiobe-index/}}, that take into account other
factors besides lines of code production, currently (2023) rank it as the seventh,
although it was also the most popular language in 2014. It can
be claimed, then, that it is among the most popular, if not the most popular
language in current development.

Due to its popularity and the fact that it has a
continuously evolving standard \cite{ecma1999262}, traditionally, there have
been different virtual machines (or interpreters) to run its programs. During
the first years, browsers were the only running platform available; however, the
introduction of Node.js running on the V8 JavaScript Engine \cite{5617064} gave
it the popularity it has today; this popularity, in turn, provoked new
interpreters to spawn like {\sf deno} \cite{runtimeintroducing} (written in
Rust) and {\sf bun} \cite{bun} programmed in the relatively unknown language
Zig.

No wonder, then, that JavaScript is also a popular language for implementing
metaheuristics, especially evolutionary algorithms, since early implementations
in the browser \cite{smith1996ga,jesusIWANN99,langdon2004global}, whole
No wonder, then, that JavaScript is also a popular language for implementing
libraries \cite{EvoStar2014:jsEO}, through complete implementations geared
towards volunteer computing \cite{2016arXiv160101607M}. But one of the
criticisms leveraged towards these implementations of evolutionary algorithms
is the (possible) lack of speed of the language compared to other compiled
languages (mainly Java, very popular with metaheuristics implementations, or
C++).

This is why, since implementation matters \cite{DBLP:conf/iwann/MereloRACML11},
choosing the right interpreter is going to have a big impact on the performance
of any workload; if you decide to choose JavaScript for any reason (such as
seamless client/server integration, or be able to run your algorithm either on
the browser or from the command line if desired) knowing which VM delivers the
best performance is essential, either from the scientific point of view, or
from the software engineering point of view.

At the same time, with the advent of the concept of green computing
\cite{kurp2008green}, it becomes increasingly important to measure not only the
raw wall clock performance (which was the focus of papers such as
\cite{DBLP:conf/evoW/GuervosBCRGRVHR17}), but also to achieve a certain level
of performance with a certain amount of energy consumption, or else to minimize
the consumption needed to run a certain workload. This will be the main focus
of this paper; since the core of the different JS virtual machines is
different, and are created with languages with different focus (Rust is focused
on memory safety \cite{noseda2022rust}, Zig based on simplicity and performance
\cite{zig}), different consumption should be expected. Since all three
languages can (roughly) run the same, unmodified source code, what we intend
with this paper is to advise on which JS interpreter might give the lowest
power consumption, the maximum performance, or both, so that evolutionary
algorithm practitioners can target it for their development.

The rest of the paper follows this plan: next we will present the state of the
art; then we will describe the experimental setup in Section \ref{sec:setup};
results will be presented next in Section \ref{sec:res}, and we will end with a
discussion of results, conclusions and future lines of work.

\section{State of the art}

% First, CPU and general algorithmic impact on power consumption

The power efficiency of CPUs (computations per kilowatt-hour) has
doubled roughly every year and a half from 1946 to 2009
\cite{koomey2011web}, this improvement has been mainly a by-product
of Moore's law, the trend of chip manufacturers to decrease in half
the size and distance between transistors every two
years. Unfortunately, it is expected that physical limits of
electronics will slow down this miniaturization in the near future.
Nonetheless, energy efficiency is becoming the most important metric
of performance and selling point in hardware development, and it is an
important driver for current innovation. The challenge of building
more power-efficient systems, can be addressed at the hardware and
software levels. In the software level, developers focus their
attention on the energy consumption of software, proposing
optimizations for more energy-efficient algorithm implementations.
Algorithm comparatives nowadays include power efficiency as a
performance metric, these include encryption algorithms
\cite{mota2017comparative,thakor2021lightweight}, estimation models
for machine learning applications \cite{garcia2019estimation} and
genetic programming (GP) \cite{diaz2018fuzzy}, and code refactoring
\cite{ournani2021tales}. Since metaheuristics are so extensively used
in machine learning applications, its interest in research has grown
in parallel to its number of applications. Many papers focus on
analyzing how certain metaheuristics parameters have an impact on
energy consumption.  Díaz-Álvarez et al. \cite{diaz2022population}
studies how the populations size of evolutionary algorithms (EAs)
influences power consumption. In an earlier work centered on genetic
algorithms (GAs) \cite{10.1007/978-3-030-45715-0_8} power-consumption
of battery-powered devices was measured for various parameter
configurations including chromosome and population sizes. The
experiments used the one max and trap function benchmark problems,
they concluded that execution time and energy consumption do not
linearly correlate and there is a connection between the GA parameters
and power consumption.  In GAs, the mutation operator appears to be a
power-hungry component according to Abdelhafez et
al. \cite{abdelhafez2019component}, in their paper they also report
that in a distributed evaluation setting, the communication scheme has
a grater impact. Fernández de Vega et
al. \cite{10.1007/978-3-319-45823-6_51} experimented with different
parameters for a GP algorithm and concluded that hand-held devices and
single-board computers (SBCs) required an order of magnitude less
energy to run the same algorithm.


\section{Methodology and experimental setup}
\label{sec:setup}

There are many ways to measure the consumption of applications running in a
computer; besides measuring directly from the power intake, those running as
applications and tapping the computer sensors fall roughly into two fields:
power monitors and energy profilers \cite{cruz21}. Power monitors need
additional hardware to measure the power drawn by the whole machine;
besides being expensive, their setup is difficult, and it is complicated 
to measure precisely how much a specific process consumes.

On the other hand, energy profilers are programs that draw information from
hardware sensors \cite{sinha2001jouletrack}, generally exposed through kernel
calls or higher-level wrapper libraries, to pinpoint consumption during a period
of time of by specific processes. Tools that give these measures, either with a
graphic or a command line interface, have been available for some time already,
and have become more popular lately. One of the mainstream processor architectures,
Intel, includes an interface called RAPL, or Running Average Power Limit
\cite{rapl}. Essentially, it consists of a series of machine-specific registers
(MSRs) that contain information on the wattage drawn by different parts of the
architecture; the content of these registers will be processed (through the
corresponding library) and consumed by different command line utilities. We will
use these command line utilities since they produce an output that can be
automatically processed and evaluated, which is what we are looking for in this
paper\footnote{Systems based on the AMD architecture have a similar power
  profiling system called AMP with its corresponding command line
  tool. However, we found that it was not well documented, and excessively
  complicated for the purposes of this article. Although not as complete, AMD
  processors also include the aforementioned MSRs so RAPL-based utilities can
  run on them}.

Energy profiling, as measured by RAPL or other APIs, includes different
\emph{domains} \cite{khan2015energy}, essentially the computing devices or
peripherals requiring the reported amount of energy.
DRAM or dynamic RAM, CORE, or GPU will report what happens on those specific devices,
with a core being every one of the computing units within the central processing
unit; other domains, like PKG or package, will report what happens in the
``package'', or CPU together with other devices in the chipset.

Considering that the available system has an AMD architecture, which is
roughly compatible with the RAPL architecture, we will use two command line
utilities, as they are the only ones available that can wrap the execution of a command
and report on consumption for that specific command. These are open source tools
that can be obtained directly from the corresponding repositories or by downloading and
compiling their source code. \begin{itemize}
\item {\sf pinpoint}~\cite{9307947} (available from
  \url{https://github.com/osmhpi/pinpoint}) is a tool that uses the RAPL
  interface, as well as the NVIDIA registers, to report the power consumed by
  these devices. In this paper, we will use it since it is the only one out of
  the three tools that can show the GPU consumption.
\item {\sf
    perf}\footnote{\url{https://perf.wiki.kernel.org/index.php/Main_Page}}~\cite{treibig2010likwid} is a
  system tool that measures all kinds of performance events, including power
  consumption. It will be used mainly for the  \texttt{pkg} domain; this is a
  domain that it is measured by all tools, but all of them would process device
    readings differently, so it will provide us with alternative estimations of
    the consumption. % it will provide (the domain? of the tools?)
    % The tool will provide a different estimation from the MSR reading; this is
    % what I understood, at least. - JJ
\item {\sf likwid-powermeter}\footnote{\url{https://github.com/RRZE-HPC/likwid}}
  measures the \texttt{CORE} as well as the \texttt{PKG} domain, which includes
  the former together with the so-called ``uncore'' components.
\end{itemize}

On the JavaScript side, we used three different interpreters:\begin{itemize}
\item \texttt{bun} version 0.5.8
\item \texttt{deno} version 1.32.1, which includes the v8 library version
  11.2.214.9 and typescript 5.0.2
\item  \texttt{node.js} version 18.5.0
\end{itemize}

\texttt{bun} and \texttt{nodejs} are fully compatible, so they run exactly the
same code. The code for \texttt{deno} needed a small modification: the path to
the library had to be changed (since it does not use the \texttt{node\_modules}
to host installed modules), and it uses a different library for processing 
the command line arguments. Other than that, the business logic was exactly the same.

These were running in an Ubuntu version 20.04.1 with kernel version
5.15.0-69. The processor is an AMD Ryzen 9 3950X 16-Core. Since we will
not be testing in a pure Intel architecture, the complete RAPL API is
not going to be available; that is also why we will be experimenting
with different tools so that we can have an adequate coverage
of energy consumption for the commands we will be measuring.

A Perl script was created to perform the experiments; it launched the scripts
and collected results by analyzing the standard output and putting it in a CSV
format that would allow examination of the experiments.

The initial experiment consisted in a script that used the \texttt{saco-js}
library to perform the union of ``bags'', sets that can hold several copies of
the same item. 1024 sets were generated; these sets had 1024, 2048, 4096
elements. Then, a union of bags was performed on pairs of sets until
there was only one left. This is similar to some operations performed by
evolutionary algorithms, mainly related to merging populations. They do not
involve floating point operations in any way.

The scripts to launch every kind of tool were slightly different, mainly because
the output needed to be processed in different ways (and different kinds of
information extracted). Additionally, \texttt{pinpoint}, which is the only tool
that does not need superuser privileges, sometimes returned 0 in energy
measures. This was an error, and those runs were discarded.

Finally, the scripts performed an additional task: since it is not possible to
disaggregate the readings for our program from the energy consumed by other
processes running at the same time, what we did was to run every program 15
times, compute the average time, and then use the same tool to measure the
energy consumption for the \texttt{sleep} program during the average amount of
time. The energy readings shown are the result of subtracting this measurement
from every one of the 15 other measurements taken, so that we can analyze
the differential of energy that has been consumed by our programs; the result is clipped at 0, since negative energy differentials would make no sense.

<<r pinpoint, echo=F>>=
library(ggplot2)
test.pinpoint <- read.csv("../code/data/pinpoint-sets-29-Mar-21-06-12.csv")
@

Experimenting with this simple program will mainly allow us to calibrate the
different tools in order to pick only one, if possible, as well as validate the
program and iron out all possible errors in the program itself or in the
processing scripts.

<<r pinpoint.gpu, echo=F, fig.cap="Boxplot of measurements of energy expenditures by the GPU in the sets problem. GPU energy consumption is measured in Joules.\\protect\\label{fig:gpu}">>=
test.pinpoint$size <- as.factor(test.pinpoint$size)
ggplot(test.pinpoint, aes(x=size,y=GPU,group=size))+geom_boxplot()
@

One of the first observations we can draw from these initial experiments is whether it is interesting to measure how much energy the GPU spends. Since \texttt{pinpoint} is the only tool able to measure this, we will use it. We show the results in Figure ref{fig:gpu}. What we see here is that it is mostly independent of the set size, but most importantly, that it is 0 in most cases, more than 50\%. The rest must be essentially noise, and it amounts to a few Joules anyway. We can probably discard \texttt{pinpoint} then as a tool, since it only offers the measurement of this device over the others, and this is not really useful. Adding this to the fact that measurements fail in many cases, we can discard it for the rest of the experiment.

\section{Experimental results}
\label{sec:res}

\section{Conclusions}
\label{sec:conc}

\section*{Acknowledgements}

This work is supported by the Ministerio espa\~{n}ol de Econom\'{\i}a y
Competitividad (Spanish Ministry of Competitivity and Economy) under project
PID2020-115570GB-C22 (DemocratAI::UGR).


\bibliographystyle{apalike}
{\small
\bibliography{energy,javascript,geneura}}


\end{document}

