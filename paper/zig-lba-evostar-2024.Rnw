\documentclass[runningheads]{llncs}

\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{color}
\renewcommand\UrlFont{\color{blue}\rmfamily}

\newcommand{\pinp}{\texttt{pinpoint}}
\newcommand{\lik}{\texttt{likwid}}

\begin{document}
\title{Best practices for energy-thrifty evolutionary algorithms in the low-level language {\sf zig}}

\author{
  Juan J. Merelo-Guervós\inst{1}\orcidID{0000-0002-1385-9741} \and Antonio M. Mora\inst{2}\orcidID{0000-0003-1603-9105} \and Mario García-Valdez\inst{3}\orcidID{0000-0002-2593-1114}
}
\institute{Department of Computer Engineering, Automatics and Robotics and CITIC University of Granada, Granada, Spain \and 
Department of Signal Theory, Telematics and Communications, University of Granada, Granada, Spain \and
Department of Graduate Studies, National Technological Institute of Mexico, Tijuana, Mexico\\
\email{jmerelo@ugr.es, amorag@ugr.es, mario@tectijuana.edu.mx}
}

\maketitle

\begin{abstract}
The most fruitful way of making evolutionary algorithms spend the least amount of energy is to consider all possible programming techniques and platform choices that could, theoretically, affect performance, and carry out experiments using EA workloads in different platforms, eventually choosing those techniques that yield the minimum amount of energy expenses. These techniques include a choice of different data structures, as well as affecting compilation in such a way that energy footprint is reduced; they have to be replicated in different computing platforms because these expenditures may be affected by all the layers of the operating system and runtime framework used. In this paper we will experiment with different data structures and code refactoring techniques in the low-level language {\sf zig}, trying to design rules of thumb that will help developers create {\em green} evolutionary algorithms. We will include two different hardware platforms, looking for the one that spends the least energy.
\keywords{Green computing, metaheuristics, energy-aware computing, evolutionary algorithms, zig}
\end{abstract}

A straightforward methodology for measuring energy consumption of EA implementations with the objective of reducing it would consider a baseline implementation and then change the code at different levels, measuring the resulting energy expenses. In \cite{DBLP:conf/icsoft/GuervosGC23}, the main factor under study was the different interpreters used in a high-level language, JavaScript. In this paper, we will focus on different techniques applied to the low-level language {\sf zig}, a language that emphasizes safety and maintainability \cite{friesen2023designing}, and that has as a motto "no implicit memory allocation," unlike other languages like C or C++, that will allocate memory without the user noticing. This strict memory management has several implications in terms of programming, but also gives the programmer more control over how and when memory is allocated and deallocated.
% I though that in C at least you have to call malloc to allocate memory, but I'm not sure. - Mario
% Yes, but it can be allocated inside a block and then no one takes care of deallocating it. You need to use allocators in zig, and it has an idiom, defer, to deallocate any data structure the moment it's allocated so that it's always allocated and de-allocated in the same block. If a function needs to allocate memory, it needs to be handled an allocator. Also, memory allocation is always typed.

In this paper, we will work on a generic evolutionary algorithm workload, and see what the impact of different choices will have on its energy consumption. With this, we will try to find some best practices that will help practitioners implement evolutionary algorithms in {\sf zig}, hopefully extensible to other low-level languages (which could include C and C++, but also Rust or Go).

The experiment setup will match the one used in \cite{DBLP:conf/icsoft/GuervosGC23}, using the same tools for energy profiling ({\sf pinpoint}) as well as Perl scripts to run the experiments and process the results. This way we can easily compare the results, but also use an established and proved methodology.

The experiments for this paper will be carried out in two different platforms:\begin{itemize} 
\item A Linux machine {\tt 5.15.0-94-generic \#104~20.04.1-Ubuntu SMP} using AMD Ryzen 9 3950X 16-Core Processor.
\item An M1 MacBook Air  with 16GB of RAM and macOS Ventura 13.2.1.
\end{itemize}

We use {\sf zig}  version 0.11.0, released by August 3, 2023, which is the last stable one at the time of writing this paper. The {\sf pinpoint} tool has no versions, but we have used one compiled from source and commit hash {\tt 1578db0}. Outputs of {\sf pinpoint} are processed by Perl scripts that generate CSV files that are then processed and plotted using R embedded in the source code of this paper. This paper's code, data, and source are available at \url{https://github.com/JJ/energy-ga-icsoft2023} under a free license.

There are several units whose consumption can be measured using {\sf pinpoint} via the RAPL interface; since the use of GPU is negligible in these examples, only memory and CPUs will be measured. Together, they are called the {\em package} (alongside caches and memory controllers); this is usually represented by the acronym {\sf PKG}. In the case of the Mac, this measurement is divided into three parts: the "E" (efficiency) and "P" (performance) CPUs, and the memory. Again, these user-initiated processes run on the "P" CPUs, so that will be the one we will be measuring.

By default, all programs will be generated using the {\tt .ReleaseFast} compile option, that optimizes performance, but also energy consumption. We will use the {\tt page\_allocator} that allocates memory in the heap. This is an efficient allocator, but also the default choice.
% The ReleaseFast option begins with a dot? - Mario
% Yes :-)

We will be examining options in different areas\begin{itemize}
\item Several data structures used in the implementation of EAs will be checked for: the default string, arrays of Boolean values and bit sets. - Mario
% Or at the beginning: ... options in different implementation areas of EAs - Mario
\item The default crossover operator used an allocator to create temporary copies of chromosomes. Several implementations will be tested.
% In this case, we knoy that allocation of memory is a very expensive operation, so we will refactor the code to avoid it? - Mario
% I had the idea that erasing or replacing data in memory was more expensive than allocating new memory. - Mario
% It worked relatively well in Linux, but it had a big penalty in the Mac. But I'm not saying here what we are going to do, leaving it for later.
\item Unlike other languages, {\sf zig} provides different memory allocators, which the developer can choose. By default, a page allocator is used, but there is the possibility of using a fixed buffer size allocator.
\item We will check the behavior of these on two different platforms, Linux (with AMD CPU) and MacOS (with M1).
\end{itemize}

We will first generate 40000 chromosomes of size 512, 1024 and 2048, and measure the energy consumption and running time of this operation; every combination is run 15 times. 
% Why so many?  This is a load that can be measured more easily?  - Mario
% Yes, essentially. It's also the same amount we've used so far. Initially it took seconds, we're now down a few hundred milliseconds... - JJ
<<evostar.zig.gen, echo=F, message=F, fig.pos="h!tb", fig.height=3, fig.cap="Average running time and PKG energy consumption generating 40K chromosomes for the different parametrizations used (represented with different colors); dot shape represents the chromosome size.">>=
library(ggplot2)

generate.chromosomes <- read.csv("../data/evostar-zig-gen-19-Mar-09-31-21.csv")
generate.chromosomes$size <- as.factor(generate.chromosomes$size)

generate.chromosomes.bool <- read.csv("../data/evostar-zig-gen-bool-19-Mar-09-30-40.csv")
generate.chromosomes.bool$size <- as.factor(generate.chromosomes.bool$size)

generate.chromosomes.bitset <- read.csv("../data/evostar-zig-bitset-gen-20-Mar-12-18-47.csv")
generate.chromosomes.bitset$size <- as.factor(generate.chromosomes.bitset$size)

generate.chromosomes.mac <- read.csv("../data/evostar-mac-gen-19-Mar-11-05-41.csv")
generate.chromosomes.mac$PKG <- generate.chromosomes.mac$ECPU + generate.chromosomes.mac$PCPU + generate.chromosomes.mac$RAM
generate.chromosomes.mac$size <- as.factor(generate.chromosomes.mac$size)

generate.chromosomes.mac.bool <- read.csv("../data/evostar-mac-gen-bool-19-Mar-10-33-29.csv")
generate.chromosomes.mac.bool$PKG <- generate.chromosomes.mac.bool$ECPU + generate.chromosomes.mac.bool$PCPU + generate.chromosomes.mac.bool$RAM
generate.chromosomes.mac.bool$size <- as.factor(generate.chromosomes.mac.bool$size)

ggplot(generate.chromosomes, aes(x=seconds, y=PKG, color="Baseline", shape=size)) +
  geom_point() +
  geom_point(data=generate.chromosomes.bitset, aes(x=seconds, y=PKG, color="BitSet", shape=size)) +
  geom_point(data=generate.chromosomes.bool, aes(x=seconds, y=PKG, color="Boolean", shape=size)) +
  geom_point(data=generate.chromosomes.mac, aes(x=seconds, y=PKG, color="Mac Baseline", shape=size)) +
  geom_point(data=generate.chromosomes.mac.bool, aes(x=seconds, y=PKG, color="Mac Boolean", shape=size)) +
  scale_color_manual(values=c("black", "red", "blue", "green","pink")) +
  labs(title="Running time and PKG energy consumption generating 40K chromosomes",
       x="Running time", y="PKG (Joules)") +
  theme_minimal()
@

Figure \ref{fig:evostar.zig.gen} represents energy consumption, as well as time taken, for the different configurations. The first thing to notice, in the upper right corner, is that using a bitset in {\sf zig} will not represent any energy saving, with a big difference in time as well as energy consumption; this is why it has not even been tested for the Mac. Second feature that stands out is that consumption as well as time is very different for the Mac, which is an entry-level laptop computer, and the AMD desktop computer. This is probably expected, but the fact is that the MacBook Air, which is a generation behind current commercial offerings, takes 25\% of the time and 10\% of the energy to do the same amount of work. While increase in energy consumption with chromosome size is quite steep in the AMD architecture used by the desktop computer, that is not the case for the Mac, with a very small increase from the smallest to the largest size.
% The first sentence is a little bit confusing. - Mario
% Maybe it's clearer now - JJ
Another observation is that, except for using bitsets as data structures, there does not seem to be a significant difference between using either strings or Boolean arrays in neither architecture, at least for this baseline operation. However, generating chromosomes is done essentially once, so it is not the most significant operation in EAs.

Thus, we will now run an experiment that, after generating the 40K chromosomes, will perform crossover + mutation + ONEMAX operations on chromosomes of size 512, 1024 and 2048 using the strings and Boolean arrays in the two architectures used above.

<<evostar.combined.ops.base, echo=F, message=F, fig.pos="h!tb", fig.height=3, fig.cap="Boxplot of PKG energy consumption processing 40K chromosomes via crossover, mutation and ONEMAX for different combinations of optimization techniques and platforms in Zig">>=
library(dplyr)

generate.chromosomes %>% group_by( size ) %>% summarise( mean.pkg = mean( PKG ), mean.seconds = mean( seconds ) ) -> generate.chromosomes.avg
generate.avgs.seconds <- c(rep( generate.chromosomes.avg$mean.seconds[1], 15),
                               rep( generate.chromosomes.avg$mean.seconds[2], 15),
                               rep( generate.chromosomes.avg$mean.seconds[3], 15))
generate.avgs.pkg <- c(rep( generate.chromosomes.avg$mean.pkg[1], 15),
                           rep( generate.chromosomes.avg$mean.pkg[2], 15),
                           rep( generate.chromosomes.avg$mean.pkg[3], 15))

combined.ops <- read.csv("../data/evostar-zig-ops-19-Mar-09-32-38.csv")
combined.ops$size <- as.factor(combined.ops$size)

combined.ops$diff.seconds <- combined.ops$seconds - generate.avgs.seconds
combined.ops$diff.PKG <- combined.ops$PKG - generate.avgs.pkg

generate.chromosomes.bool %>% group_by( size ) %>% summarise( mean.pkg = mean( PKG ), mean.seconds = mean( seconds ) ) -> generate.chromosomes.bool.avg
generate.bool.avgs.seconds <- c(rep( generate.chromosomes.bool.avg$mean.seconds[1], 15),
                               rep( generate.chromosomes.bool.avg$mean.seconds[2], 15),
                               rep( generate.chromosomes.bool.avg$mean.seconds[3], 15))
generate.bool.avgs.pkg <- c(rep( generate.chromosomes.bool.avg$mean.pkg[1], 15),
                           rep( generate.chromosomes.bool.avg$mean.pkg[2], 15),
                           rep( generate.chromosomes.bool.avg$mean.pkg[3], 15))

combined.ops.bool <- read.csv("../data/evostar-zig-ops-bool-19-Mar-09-29-42.csv")
combined.ops.bool$size <- as.factor(combined.ops.bool$size)

combined.ops.bool$diff.seconds <- combined.ops.bool$seconds - generate.bool.avgs.seconds
combined.ops.bool$diff.PKG <- combined.ops.bool$PKG - generate.bool.avgs.pkg

generate.chromosomes.mac %>% group_by( size ) %>% summarise( mean.pkg = mean( PKG ), mean.seconds = mean( seconds ) ) -> generate.chromosomes.mac.avg
generate.mac.avgs.seconds <- c(rep( generate.chromosomes.mac.avg$mean.seconds[1], 15),
                               rep( generate.chromosomes.mac.avg$mean.seconds[2], 15),
                               rep( generate.chromosomes.mac.avg$mean.seconds[3], 15))
generate.mac.avgs.pkg <- c(rep( generate.chromosomes.mac.avg$mean.pkg[1], 15),
                           rep( generate.chromosomes.mac.avg$mean.pkg[2], 15),
                           rep( generate.chromosomes.mac.avg$mean.pkg[3], 15))

combined.ops.mac <- read.csv("../data/evostar-mac-ops-19-Mar-10-39-59.csv")
combined.ops.mac$PKG <- combined.ops.mac$ECPU + combined.ops.mac$PCPU + combined.ops.mac$RAM
combined.ops.mac$size <- as.factor(combined.ops.mac$size)
combined.ops.mac$diff.seconds <- combined.ops.mac$seconds - generate.mac.avgs.seconds
combined.ops.mac$diff.PKG <- combined.ops.mac$PKG - generate.mac.avgs.pkg
combined.ops.mac$Platform <- combined.ops.mac$Prefix

generate.chromosomes.mac.bool %>% group_by( size ) %>% summarise( mean.pkg = mean( PKG ), mean.seconds = mean( seconds ) ) -> generate.chromosomes.mac.bool.avg
generate.mac.bool.avgs.seconds <- c(rep( generate.chromosomes.mac.bool.avg$mean.seconds[1], 15),
                               rep( generate.chromosomes.mac.bool.avg$mean.seconds[2], 15),
                               rep( generate.chromosomes.mac.bool.avg$mean.seconds[3], 15))
generate.mac.bool.avgs.pkg <- c(rep( generate.chromosomes.mac.bool.avg$mean.pkg[1], 15),
                           rep( generate.chromosomes.mac.bool.avg$mean.pkg[2], 15),
                           rep( generate.chromosomes.mac.bool.avg$mean.pkg[3], 15))
combined.ops.mac.bool <- read.csv("../data/evostar-mac-ops-bool-19-Mar-10-36-18.csv")
combined.ops.mac.bool$PKG <- combined.ops.mac.bool$ECPU + combined.ops.mac.bool$PCPU + combined.ops.mac.bool$RAM
combined.ops.mac.bool$size <- as.factor(combined.ops.mac.bool$size)
combined.ops.mac.bool$diff.seconds <- combined.ops.mac.bool$seconds - generate.mac.bool.avgs.seconds
combined.ops.mac.bool$diff.PKG <- combined.ops.mac.bool$PKG - generate.mac.bool.avgs.pkg
combined.ops.mac.bool$Platform <- combined.ops.mac.bool$Prefix

all.combined.ops <- data.frame( Platform = c( combined.ops.mac$Platform, combined.ops.mac.bool$Platform, combined.ops$Platform, combined.ops.bool$Platform ),
                                size = c( combined.ops.mac$size, combined.ops.mac.bool$size, combined.ops$size, combined.ops.bool$size ),
                                diff.seconds = c( combined.ops.mac$diff.seconds, combined.ops.mac.bool$diff.seconds, combined.ops$diff.seconds, combined.ops.bool$diff.second ),
                                diff.PKG = c( combined.ops.mac$diff.PKG, combined.ops.mac.bool$diff.PKG, combined.ops$diff.PKG, combined.ops.bool$diff.PKG ) )

ggplot( data = all.combined.ops, aes( x = size, y = diff.PKG, fill=Platform ) ) +
  geom_boxplot() +
  labs( title = "Energy consumption difference between different compilation policies for zig", y = "Energy consumption difference (seconds)" ) +
  theme_minimal()
@

What Figure \ref{fig:evostar.combined.ops.base} shows is a remarkable difference in energy consumption in the Mac platform, which goes against the big difference shown in the generation of chromosomes. This probably reveals a problem in the implementation of some feature used by the evolutionary algorithm. Digging into the code, we found that the main issue was the need to use allocation within the crossover operator. A refactoring, which eliminated this need to use allocators, was in order, yielding the results shown in Figure \ref{fig:evostar.combined.ops.refactor}. The energy consumption is now more in line with the generation of the chromosomes, and the difference between the different compilation policies is more in line with the previous results.
% What was the refactoring? - Mario
<<evostar.combined.ops.refactor, echo=F, message=F, fig.pos="h!tb", fig.height=3, fig.cap="Boxplot of PKG energy consumption processing 40K chromosomes via crossover, mutation and ONEMAX after crossover has been refactored">>=

combined.ops.noalloc <- read.csv("../data/evostar-zig-ops-nx-noalloc-20-Mar-09-46-37.csv")
combined.ops.noalloc$size <- as.factor(combined.ops.noalloc$size)
combined.ops.noalloc$diff.seconds <- combined.ops.noalloc$seconds - generate.avgs.seconds
combined.ops.noalloc$diff.PKG <- combined.ops.noalloc$PKG - generate.avgs.pkg
combined.ops.noalloc$Platform <- "Base"

combined.ops.noalloc.bool <- read.csv("../data/evostar-zig-bool-ops-nx-noalloc-20-Mar-09-45-48.csv")
combined.ops.noalloc.bool$size <- as.factor(combined.ops.noalloc.bool$size)
combined.ops.noalloc.bool$diff.seconds <- combined.ops.noalloc.bool$seconds - generate.bool.avgs.seconds
combined.ops.noalloc.bool$diff.PKG <- combined.ops.noalloc.bool$PKG - generate.bool.avgs.pkg
combined.ops.noalloc.bool$Platform <- "Bool"

combined.ops.mac.noalloc <- read.csv("../data/evostar-zig-mac-ops-nx-noalloc-20-Mar-09-52-13.csv")
combined.ops.mac.noalloc$PKG <- combined.ops.mac.noalloc$ECPU + combined.ops.mac.noalloc$PCPU + combined.ops.mac.noalloc$RAM
combined.ops.mac.noalloc$size <- as.factor(combined.ops.mac.noalloc$size)
combined.ops.mac.noalloc$diff.seconds <- combined.ops.mac.noalloc$seconds - generate.mac.avgs.seconds
combined.ops.mac.noalloc$diff.PKG <- combined.ops.mac.noalloc$PKG - generate.mac.avgs.pkg
combined.ops.mac.noalloc$Platform <- "Mac Base"

combined.ops.mac.noalloc.bool <- read.csv("../data/evostar-zig-mac-bool-ops-nx-noalloc-20-Mar-09-50-15.csv")
combined.ops.mac.noalloc.bool$PKG <- combined.ops.mac.noalloc.bool$ECPU + combined.ops.mac.noalloc.bool$PCPU + combined.ops.mac.noalloc.bool$RAM
combined.ops.mac.noalloc.bool$size <- as.factor(combined.ops.mac.noalloc.bool$size)
combined.ops.mac.noalloc.bool$diff.seconds <- combined.ops.mac.noalloc.bool$seconds - generate.mac.bool.avgs.seconds
combined.ops.mac.noalloc.bool$diff.PKG <- combined.ops.mac.noalloc.bool$PKG - generate.mac.bool.avgs.pkg
combined.ops.mac.noalloc.bool$Platform <- "Mac Bool"

all.combined.ops.noalloc <- data.frame( Platform = c( combined.ops.mac.noalloc$Platform, combined.ops.mac.noalloc.bool$Platform, combined.ops.noalloc$Platform, combined.ops.noalloc.bool$Platform ),
                                size = c( combined.ops.mac.noalloc$size, combined.ops.mac.noalloc.bool$size, combined.ops.noalloc$size, combined.ops.noalloc.bool$size ),
                                diff.seconds = c( combined.ops.mac.noalloc$diff.seconds, combined.ops.mac.noalloc.bool$diff.seconds, combined.ops.noalloc$diff.seconds, combined.ops.noalloc.bool$diff.seconds ),
                                diff.PKG = c( combined.ops.mac.noalloc$diff.PKG, combined.ops.mac.noalloc.bool$diff.PKG, combined.ops.noalloc$diff.PKG, combined.ops.noalloc.bool$diff.PKG ) )

ggplot( data = all.combined.ops.noalloc, aes( x = size, y = diff.PKG, fill=Platform ) ) + 
  geom_boxplot() +
  labs( title = "Energy consumption difference between different compilation policies for zig after refactoring", y = "Energy consumption difference (PKG)" ) +
  theme_minimal()
@

Figure \ref{fig:evostar.combined.ops.refactor} shows the energy consumption after refactoring. Both platforms show a considerable improvement, but it is more dramatic in the case of the Mac platform. We can anyway observe that, although there is a certain difference between both data structures, string and Boolean array, that difference does not hold across platforms and sizes. Strings seem to best the other option in more ocassions, but when the size is the biggest, it might be at a certain disadvantage. It is probably the case that the amount of memory used by any of them is the same, and the random-access structure is also similar in performance and energy consumption (and unlike bitsets, which probably used more memory, since they internally used structs). On the other hand, eliminating allocation from the crossover operator results in energy savings across the board, although they are much more dramatic in the Mac platform, consuming energy for 40k evaluations that is a fraction of a Joule, and almost two orders of magnitude less of what the desktop computer consumes. 


This leads us to the conclusion, carried over from other papers, that rather than taking assumptions on the behavior of implementations based on first principles or knowledge of the computing platform, we need to always create energy profiles of the implementations, and measure for different data structures and across refactoring of the evolutionary algorithm code.

It seems clear, anyway, that platforms that emphasize energy savings like the M1 chip used by the MacBookAir will use much less energy, dramatically so in some cases, than desktop solutions based on AMD. This does not extend to the operating system implementation itself: operations that depend on it, such as memory allocation, will have a different impact on the energy profile, which might be one of the reasons they should be minimized whenever possible. Fortunately, {\sf zig} is a language and toolchain with very strict control over memory allocation, allowing us to be very conscious over where it could be eliminated, as we have done in this case.

As a final conclusion, implementing an EA in {\sf zig} and running in on a Mac may result in an improvement of several orders of magnitude in consumption of energy over using high-level platforms (like Python or Javascript) and desktop machines. If we strive for greener computing, we should really consider them for our experiments. Although {\sf zig} cannot be considered mainstream right now, its performance and capability should make it a very interesting option for the future.


\section*{Acknowledgements and data availability}

This work is supported by the Ministerio espa\~{n}ol de Econom\'{\i}a y
Competitividad (Spanish Ministry of Competitivity and Economy) under project
PID2020-115570GB-C22 (DemocratAI::UGR). We are also very grateful to the {\sf zig} community. Source and data available from \url{https://github.com/JJ/energy-ga-icsoft-2023} under a GPL license.

\bibliographystyle{splncs04}

\bibliography{energy,javascript,geneura}


\end{document}

